{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 第七章 网络优化与正则化\n",
    "\n",
    "### 1.<span style=\"color:red\">(必修题)</span>\n",
    "    在第五章（上）《卷积神经网络理论解读》中，我们基于LeNet网络实现了手写体数字识别实验。在本实践中，我们重复该实验，以最初的试验结果作为基线（Baseline），运用在本章内学到的网络优化方法进行调优，尝试提升模型精度。\n",
    "    a) 实验baseline：给出代码，跑出baseline结果为0.895\n",
    "    b) 尝试修改学习率、批大小、更换优化器、增加训练轮数、增加学习率衰减、学习率预热等策略来提升模型精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train/dev/test set:1000/200/200\n"
     ]
    }
   ],
   "source": [
    "import json\r\n",
    "import gzip\r\n",
    "\r\n",
    "# 打印并观察数据集分布情况\r\n",
    "train_set, dev_set, test_set = json.load(gzip.open('./mnist.json.gz'))\r\n",
    "train_images, train_labels = train_set[0][:1000], train_set[1][:1000]\r\n",
    "dev_images, dev_labels = dev_set[0][:200], dev_set[1][:200]\r\n",
    "test_images, test_labels = test_set[0][:200], test_set[1][:200]\r\n",
    "train_set, dev_set, test_set = [train_images, train_labels], [dev_images, dev_labels], [test_images, test_labels]\r\n",
    "print('Length of train/dev/test set:{}/{}/{}'.format(len(train_set[0]), len(dev_set[0]), len(test_set[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle.vision.transforms import Compose, Resize, Normalize\r\n",
    "\r\n",
    "# 数据预处理\r\n",
    "transforms = Compose([Resize(32), Normalize(mean=[127.5], std=[127.5], data_format='CHW')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "import random\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from PIL import Image\r\n",
    "import numpy as np\r\n",
    "import paddle.io as io\r\n",
    "\r\n",
    "class MNIST_dataset(io.Dataset):\r\n",
    "    def __init__(self, dataset, transforms, mode='train'):\r\n",
    "        self.mode = mode\r\n",
    "        self.transforms =transforms\r\n",
    "        self.dataset = dataset\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        # 获取图像和标签\r\n",
    "        image, label = self.dataset[0][idx], self.dataset[1][idx]\r\n",
    "        image, label = np.array(image).astype('float32'), int(label)\r\n",
    "        image = np.reshape(image, [28,28])\r\n",
    "        image = Image.fromarray(image.astype('uint8'), mode='L')\r\n",
    "        image = self.transforms(image)\r\n",
    "\r\n",
    "        return image, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 固定随机种子\r\n",
    "random.seed(0)\r\n",
    "# 加载 mnist 数据集\r\n",
    "train_dataset = MNIST_dataset(dataset=train_set, transforms=transforms, mode='train')\r\n",
    "test_dataset = MNIST_dataset(dataset=test_set, transforms=transforms, mode='test')\r\n",
    "dev_dataset = MNIST_dataset(dataset=dev_set, transforms=transforms, mode='dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1. a). 复现baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "class Paddle_LeNet(nn.Layer):\r\n",
    "    def __init__(self, in_channels, num_classes=10):\r\n",
    "        super(Paddle_LeNet, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2D(in_channels=in_channels, out_channels=6, kernel_size=5)\r\n",
    "        self.pool2 = nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.conv3 = nn.Conv2D(in_channels=6, out_channels=16, kernel_size=5)\r\n",
    "        self.pool4 = nn.AvgPool2D(kernel_size=2, stride=2)\r\n",
    "        self.conv5 = nn.Conv2D(in_channels=16, out_channels=120, kernel_size=5)\r\n",
    "        self.linear6 = nn.Linear(in_features=120, out_features=84)\r\n",
    "        self.linear7 = nn.Linear(in_features=84, out_features=num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        output = F.relu(self.conv1(x))\r\n",
    "        output = self.pool2(output)\r\n",
    "        output = F.relu(self.conv3(output))\r\n",
    "        output = self.pool4(output)\r\n",
    "        output = F.relu(self.conv5(output))\r\n",
    "        output = paddle.squeeze(output, axis=[2,3])\r\n",
    "        output = F.relu(self.linear6(output))\r\n",
    "        output = self.linear7(output)\r\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 10:55:07.290863  5050 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0727 10:55:07.295270  5050 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/10, step: 0/160, loss: 2.77382\n",
      "[Train] epoch: 0/10, step: 15/160, loss: 1.44361\n",
      "[Evaluate]  dev score: 0.41500, dev loss: 1.72289\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.41500\n",
      "[Train] epoch: 1/10, step: 30/160, loss: 0.72785\n",
      "[Evaluate]  dev score: 0.79500, dev loss: 0.56832\n",
      "[Evaluate] best accuracy performence has been updated: 0.41500 --> 0.79500\n",
      "[Train] epoch: 2/10, step: 45/160, loss: 0.58829\n",
      "[Evaluate]  dev score: 0.79500, dev loss: 0.52878\n",
      "[Train] epoch: 3/10, step: 60/160, loss: 0.49997\n",
      "[Evaluate]  dev score: 0.88000, dev loss: 0.36518\n",
      "[Evaluate] best accuracy performence has been updated: 0.79500 --> 0.88000\n",
      "[Train] epoch: 4/10, step: 75/160, loss: 0.13955\n",
      "[Evaluate]  dev score: 0.89500, dev loss: 0.26394\n",
      "[Evaluate] best accuracy performence has been updated: 0.88000 --> 0.89500\n",
      "[Train] epoch: 5/10, step: 90/160, loss: 0.11211\n",
      "[Evaluate]  dev score: 0.91500, dev loss: 0.20057\n",
      "[Evaluate] best accuracy performence has been updated: 0.89500 --> 0.91500\n",
      "[Train] epoch: 6/10, step: 105/160, loss: 0.08351\n",
      "[Evaluate]  dev score: 0.90500, dev loss: 0.19616\n",
      "[Train] epoch: 7/10, step: 120/160, loss: 0.15590\n",
      "[Evaluate]  dev score: 0.88000, dev loss: 0.23140\n",
      "[Train] epoch: 8/10, step: 135/160, loss: 0.07623\n",
      "[Evaluate]  dev score: 0.92000, dev loss: 0.16087\n",
      "[Evaluate] best accuracy performence has been updated: 0.91500 --> 0.92000\n",
      "[Train] epoch: 9/10, step: 150/160, loss: 0.07976\n",
      "[Evaluate]  dev score: 0.91000, dev loss: 0.21363\n",
      "[Evaluate]  dev score: 0.92500, dev loss: 0.21119\n",
      "[Evaluate] best accuracy performence has been updated: 0.92000 --> 0.92500\n",
      "[Train] Training done!\n"
     ]
    }
   ],
   "source": [
    "import paddle.optimizer as opt\r\n",
    "from nndl import RunnerV3, metric\r\n",
    "\r\n",
    "paddle.seed(100)\r\n",
    "# 学习率大小\r\n",
    "lr = 0.1\r\n",
    "\r\n",
    "# 批次大小\r\n",
    "batch_size = 64\r\n",
    "\r\n",
    "# 加载数据\r\n",
    "train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
    "dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)\r\n",
    "test_loader = io.DataLoader(test_dataset, batch_size=batch_size)\r\n",
    "\r\n",
    "# 定义LeNet网络\r\n",
    "model = Paddle_LeNet(in_channels=1, num_classes=10)\r\n",
    "# 定义优化器\r\n",
    "optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())\r\n",
    "# 定义损失函数\r\n",
    "loss_fn = F.cross_entropy\r\n",
    "# 定义评价指标\r\n",
    "metric = metric.Accuracy(is_logist=True)\r\n",
    "# 实例化 RunnerV3 类，并传入训练配置。\r\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\r\n",
    "# 启动训练\r\n",
    "log_steps = 15\r\n",
    "eval_steps = 15\r\n",
    "runner.train(train_loader, dev_loader, num_epochs=10, log_steps=log_steps, \r\n",
    "                eval_steps=eval_steps, save_path=\"best_baseline.pdparams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] accuracy/loss: 0.9150/0.2239\n"
     ]
    }
   ],
   "source": [
    "# 加载最优模型\r\n",
    "runner.load_model('best_baseline.pdparams')\r\n",
    "# 模型评价\r\n",
    "score, loss = runner.evaluate(test_loader)\r\n",
    "print(\"[Test] accuracy/loss: {:.4f}/{:.4f}\".format(score, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "epoch数设为10.\n",
    "复现baseline的结果为**0.9150**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1. b). 采取策略提升模型精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/30, step: 0/60, loss: 2.62919\n",
      "[Train] epoch: 7/30, step: 15/60, loss: 0.41444\n",
      "[Evaluate]  dev score: 0.84000, dev loss: 0.40736\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.84000\n",
      "[Train] epoch: 15/30, step: 30/60, loss: 0.11065\n",
      "[Evaluate]  dev score: 0.88000, dev loss: 0.27252\n",
      "[Evaluate] best accuracy performence has been updated: 0.84000 --> 0.88000\n",
      "[Train] epoch: 22/30, step: 45/60, loss: 0.02630\n",
      "[Evaluate]  dev score: 0.92500, dev loss: 0.21843\n",
      "[Evaluate] best accuracy performence has been updated: 0.88000 --> 0.92500\n",
      "[Evaluate]  dev score: 0.93000, dev loss: 0.19045\n",
      "[Evaluate] best accuracy performence has been updated: 0.92500 --> 0.93000\n",
      "[Train] Training done!\n"
     ]
    }
   ],
   "source": [
    "import paddle.optimizer as opt\r\n",
    "from nndl import RunnerV3, metric\r\n",
    "\r\n",
    "paddle.seed(100)\r\n",
    "# 学习率大小\r\n",
    "lr = 0.01\r\n",
    "\r\n",
    "# 批次大小\r\n",
    "batch_size = 512\r\n",
    "\r\n",
    "# 加载数据\r\n",
    "train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
    "dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)\r\n",
    "test_loader = io.DataLoader(test_dataset, batch_size=batch_size)\r\n",
    "\r\n",
    "# 定义LeNet网络\r\n",
    "model = Paddle_LeNet(in_channels=1, num_classes=10)\r\n",
    "\r\n",
    "# 定义优化器\r\n",
    "# optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\r\n",
    "\r\n",
    "# 定义损失函数\r\n",
    "loss_fn = F.cross_entropy\r\n",
    "# 定义评价指标\r\n",
    "metric = metric.Accuracy(is_logist=True)\r\n",
    "# 实例化 RunnerV3 类，并传入训练配置。\r\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\r\n",
    "# 启动训练\r\n",
    "log_steps = 15\r\n",
    "eval_steps = 15\r\n",
    "runner.train(train_loader, dev_loader, num_epochs=30, log_steps=log_steps, \r\n",
    "                eval_steps=eval_steps, save_path=\"best_tuned.pdparams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] accuracy/loss: 0.9400/0.1236\n"
     ]
    }
   ],
   "source": [
    "# 加载最优模型\r\n",
    "runner.load_model('best_tuned.pdparams')\r\n",
    "# 模型评价\r\n",
    "score, loss = runner.evaluate(test_loader)\r\n",
    "print(\"[Test] accuracy/loss: {:.4f}/{:.4f}\".format(score, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "初始学习率保持为0.01，批次大小调整为512，epoch数设为30，优化器选择Adam算法.\n",
    "模型的精度为**0.9400**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.<span style=\"color:red\">(附加题&加分题)</span>\n",
    "\n",
    "    在课程中我们讲到了Adam优化器及其实现方法，也了解到可以通过在损失函数中引入ℓ2正则化来缓解过拟合。但Adam优化器中自适应学习率的存在会使得ℓ2正则化失效。AdamW优化器的提出则可解决这一问题。\n",
    "    大家可通过阅读论文DECOUPLED WEIGHT DECAY REGULARIZATION详细了解AdamW优化器，并通过调用paddle.optimzer.AdamW API实现AdamW优化器指导LeNet网络在MNIST数据集上完成训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/30, step: 0/60, loss: 2.58380\n",
      "[Train] epoch: 7/30, step: 15/60, loss: 0.44040\n",
      "[Evaluate]  dev score: 0.88000, dev loss: 0.32249\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.88000\n",
      "[Train] epoch: 15/30, step: 30/60, loss: 0.09892\n",
      "[Evaluate]  dev score: 0.92000, dev loss: 0.20775\n",
      "[Evaluate] best accuracy performence has been updated: 0.88000 --> 0.92000\n",
      "[Train] epoch: 22/30, step: 45/60, loss: 0.02301\n",
      "[Evaluate]  dev score: 0.92500, dev loss: 0.25228\n",
      "[Evaluate] best accuracy performence has been updated: 0.92000 --> 0.92500\n",
      "[Evaluate]  dev score: 0.95000, dev loss: 0.22901\n",
      "[Evaluate] best accuracy performence has been updated: 0.92500 --> 0.95000\n",
      "[Train] Training done!\n"
     ]
    }
   ],
   "source": [
    "import paddle.optimizer as opt\r\n",
    "from nndl import RunnerV3, metric\r\n",
    "\r\n",
    "paddle.seed(100)\r\n",
    "# 学习率大小\r\n",
    "lr = 0.01\r\n",
    "\r\n",
    "# 批次大小\r\n",
    "batch_size = 512\r\n",
    "\r\n",
    "# 加载数据\r\n",
    "train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
    "dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)\r\n",
    "test_loader = io.DataLoader(test_dataset, batch_size=batch_size)\r\n",
    "\r\n",
    "# 定义LeNet网络\r\n",
    "model = Paddle_LeNet(in_channels=1, num_classes=10)\r\n",
    "\r\n",
    "# 定义优化器\r\n",
    "# optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=lr, parameters=model.parameters())\r\n",
    "\r\n",
    "# 定义损失函数\r\n",
    "loss_fn = F.cross_entropy\r\n",
    "# 定义评价指标\r\n",
    "metric = metric.Accuracy(is_logist=True)\r\n",
    "# 实例化 RunnerV3 类，并传入训练配置。\r\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\r\n",
    "# 启动训练\r\n",
    "log_steps = 15\r\n",
    "eval_steps = 15\r\n",
    "runner.train(train_loader, dev_loader, num_epochs=30, log_steps=log_steps, \r\n",
    "                eval_steps=eval_steps, save_path=\"best_adamw.pdparams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] accuracy/loss: 0.9650/0.1126\n"
     ]
    }
   ],
   "source": [
    "# 加载最优模型\r\n",
    "runner.load_model('best_adamw.pdparams')\r\n",
    "# 模型评价\r\n",
    "score, loss = runner.evaluate(test_loader)\r\n",
    "print(\"[Test] accuracy/loss: {:.4f}/{:.4f}\".format(score, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "初始学习率和批次大小与前面用Adam算法做优化器的模型保持一致，优化器选择AdamW算法. 模型的精度为**0.9650**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.<span style=\"color:red\">(附加题&简答题&加分题)</span>\n",
    "\n",
    "\t小明意识到了自己在搭建宠物店猫狗识别系统的过程中，在采集数据集犯下的错误。经过调整后，小明开始训练网络，发现在训练数据上损失不断下降而在验证数据上损失先降后增，请分析该现象是什么，如何缓解？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 训练中发生了过拟合。可以尝试引入$l_1$和$l_2$正则化，以及提前停止等正则化方法，减轻模型的过拟合。\n",
    "\n",
    "+ 训练集和验证集的数据分布可能相差过大。可以尝试随机打乱数据、重新划分数据集或者对实验数据进行扩充。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
