{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 第八章 注意力机制\n",
    "\n",
    "### 1.Transformer层数设置为多层，采用随机初始化位置编码，并且设置位置编码为可训练的方式，并进行文本分类实验，比较实验结果。<span style=\"color:red\">(必修题)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 14:07:20.769184  6196 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0727 14:07:20.803200  6196 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/3, step: 0/588, loss: 0.96230\n",
      "[Train] epoch: 0/3, step: 100/588, loss: 0.70999\n",
      "[Train] epoch: 1/3, step: 200/588, loss: 0.54233\n",
      "[Train] epoch: 1/3, step: 300/588, loss: 0.43434\n",
      "[Train] epoch: 2/3, step: 400/588, loss: 0.36233\n",
      "[Train] epoch: 2/3, step: 500/588, loss: 0.18218\n",
      "[Evaluate]  dev score: 0.74808, dev loss: 0.59430\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.74808\n",
      "[Evaluate]  dev score: 0.75896, dev loss: 0.64236\n",
      "[Evaluate] best accuracy performence has been updated: 0.74808 --> 0.75896\n",
      "[Train] Training done!\n",
      "[Train] epoch: 0/3, step: 0/588, loss: 0.83308\n",
      "[Train] epoch: 0/3, step: 100/588, loss: 0.62887\n",
      "[Train] epoch: 1/3, step: 200/588, loss: 0.54904\n",
      "[Train] epoch: 1/3, step: 300/588, loss: 0.35240\n",
      "[Train] epoch: 2/3, step: 400/588, loss: 0.20508\n",
      "[Train] epoch: 2/3, step: 500/588, loss: 0.12596\n",
      "[Evaluate]  dev score: 0.80272, dev loss: 0.52596\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.80272\n",
      "[Evaluate]  dev score: 0.78984, dev loss: 0.71398\n",
      "[Train] Training done!\n",
      "1-layer transformer model with learnable positional embedding: Accuracy: 0.75352\n",
      "2-layer transformer model with learnable positional embedding: Accuracy: 0.80312\n"
     ]
    }
   ],
   "source": [
    "from data import load_vocab,load_imdb_data\r\n",
    "from paddle.optimizer import Adam\r\n",
    "from nndl import Accuracy, RunnerV3\r\n",
    "import time\r\n",
    "from nndl import IMDBDataset\r\n",
    "from functools import partial\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import numpy as np\r\n",
    "from paddle.io import DataLoader\r\n",
    "\r\n",
    "train_data, dev_data, test_data = load_imdb_data(\"./dataset\") # 加载IMDB数据集和word2id词典\r\n",
    "word2id_dict= load_vocab(\"dataset/vocab.txt\") # 加载词典\r\n",
    "padding_idx=word2id_dict['[pad]']\r\n",
    "batch_size = 128\r\n",
    "max_seq_len = 128\r\n",
    "train_set = IMDBDataset(train_data, word2id_dict, max_seq_len)\r\n",
    "dev_set = IMDBDataset(dev_data, word2id_dict, max_seq_len)\r\n",
    "test_set = IMDBDataset(test_data, word2id_dict, max_seq_len)\r\n",
    "\r\n",
    "\r\n",
    "def collate_fn(batch_data, pad_val=1):\r\n",
    "    seqs, labels, lens = [], [], []\r\n",
    "    for seq, label in batch_data:\r\n",
    "        seqs.append(seq)\r\n",
    "        labels.append([label])\r\n",
    "        lens.append(len(seq))\r\n",
    "    \r\n",
    "    max_len = max(lens)\r\n",
    "    for i in range(len(seqs)):\r\n",
    "        seqs[i] = seqs[i] + [pad_val] * (max_len - len(seqs[i]))\r\n",
    "    \r\n",
    "    return (paddle.to_tensor(seqs), paddle.to_tensor(lens)),paddle.to_tensor(labels)\r\n",
    "\r\n",
    "\r\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn)\r\n",
    "dev_loader = DataLoader(dev_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn)\r\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn)\r\n",
    "\r\n",
    "class WordEmbedding(nn.Layer):\r\n",
    "    def __init__(self, vocab_size, emb_size, padding_idx=0):\r\n",
    "        super(WordEmbedding, self).__init__()\r\n",
    "        # Embedding的维度\r\n",
    "        self.emb_size = emb_size\r\n",
    "        # 使用随机正态（高斯）分布初始化 embedding\r\n",
    "        self.word_embedding = nn.Embedding(vocab_size, emb_size,\r\n",
    "            padding_idx=padding_idx, weight_attr=paddle.ParamAttr(\r\n",
    "                initializer=nn.initializer.Normal(0.0, emb_size ** -0.5) ), )\r\n",
    "\r\n",
    "    def forward(self, word):\r\n",
    "        word_emb = self.emb_size ** 0.5 * self.word_embedding(word)\r\n",
    "        return word_emb\r\n",
    "\r\n",
    "class SegmentEmbedding(nn.Layer):\r\n",
    "    def __init__(self, vocab_size, emb_size):\r\n",
    "        super(SegmentEmbedding, self).__init__()\r\n",
    "        # Embedding的维度\r\n",
    "        self.emb_size = emb_size\r\n",
    "        # 分段编码\r\n",
    "        self.seg_embedding = nn.Embedding(\r\n",
    "            num_embeddings=vocab_size, embedding_dim=emb_size\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, word):\r\n",
    "        seg_embedding = self.seg_embedding(word)\r\n",
    "        return seg_embedding\r\n",
    "\r\n",
    "def get_sinusoid_encoding(position_size, hidden_size):\r\n",
    "    \"\"\"位置编码 \"\"\"\r\n",
    "\r\n",
    "    def cal_angle(pos, hidden_idx):\r\n",
    "        # 公式里的 i = hid_idx // 2\r\n",
    "        return pos / np.power(10000, 2 * (hidden_idx // 2) / hidden_size)\r\n",
    "\r\n",
    "    def get_posi_angle_vec(pos):\r\n",
    "        return [cal_angle(pos, hidden_j) for hidden_j in range(hidden_size)]\r\n",
    "\r\n",
    "    sinusoid = np.array([get_posi_angle_vec(pos_i) for pos_i in range(position_size)])\r\n",
    "    # dim 2i  偶数正弦\r\n",
    "    # 从0开始，每隔2间隔求正弦值\r\n",
    "    sinusoid[:, 0::2] = np.sin(sinusoid[:, 0::2])\r\n",
    "    # dim 2i 1  奇数余弦\r\n",
    "    # 从1开始，每隔2间隔取余弦\r\n",
    "    sinusoid[:, 1::2] = np.cos(sinusoid[:, 1::2])\r\n",
    "    # position_size × hidden_size  得到每一个词的位置向量\r\n",
    "    return sinusoid.astype(\"float32\")\r\n",
    "\r\n",
    "class PositionalEmbedding(nn.Layer):\r\n",
    "    def __init__(self, max_length,emb_size,stop_gradient=True, random_pos=False):\r\n",
    "        super(PositionalEmbedding, self).__init__()\r\n",
    "        self.emb_size = emb_size\r\n",
    "        self.stop_gradient = stop_gradient\r\n",
    "        if random_pos:\r\n",
    "            self.pos_encoder = nn.Embedding(\r\n",
    "                num_embeddings=max_length,\r\n",
    "                embedding_dim=self.emb_size,\r\n",
    "                weight_attr=paddle.ParamAttr(\r\n",
    "                initializer=nn.initializer.Normal(0.0, emb_size ** -0.5)))\r\n",
    "        else:\r\n",
    "            self.pos_encoder = nn.Embedding(\r\n",
    "                num_embeddings=max_length,\r\n",
    "                embedding_dim=self.emb_size,\r\n",
    "                weight_attr=paddle.ParamAttr(\r\n",
    "                    initializer=paddle.nn.initializer.Assign(\r\n",
    "                        get_sinusoid_encoding(max_length, self.emb_size))))\r\n",
    "    \r\n",
    "    def forward(self, pos, stop_gradient = True):\r\n",
    "        pos_emb = self.pos_encoder(pos)\r\n",
    "        # 关闭位置编码的梯度更新\r\n",
    "        pos_emb.stop_gradient = stop_gradient\r\n",
    "        return pos_emb\r\n",
    "\r\n",
    "\r\n",
    "class TransformerEmbeddings(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    包括输入编码，分段编码，位置编码\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        vocab_size,\r\n",
    "        hidden_size=768,\r\n",
    "        hidden_dropout_prob=0.1,\r\n",
    "        position_size=512,\r\n",
    "        segment_size=256,\r\n",
    "        pos_stop_gradient=True,\r\n",
    "        random_pos=False\r\n",
    "    ):\r\n",
    "        super(TransformerEmbeddings, self).__init__()\r\n",
    "        # 输入编码向量\r\n",
    "        self.word_embeddings = WordEmbedding(vocab_size, hidden_size)\r\n",
    "        # 位置编码向量\r\n",
    "        self.pos_stop_gradient = pos_stop_gradient\r\n",
    "        self.position_embeddings = PositionalEmbedding(position_size, hidden_size, stop_gradient=self.pos_stop_gradient, random_pos=random_pos)\r\n",
    "        # 分段编码\r\n",
    "        self.segment_embeddings = SegmentEmbedding(segment_size, hidden_size)\r\n",
    "        # 层规范化\r\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\r\n",
    "        # Dropout操作\r\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\r\n",
    "\r\n",
    "    def forward(self, input_ids, segment_ids = None, position_ids = None):\r\n",
    "        if position_ids is None:\r\n",
    "            # 初始化全1的向量，比如[1,1,1,1]\r\n",
    "            ones = paddle.ones_like(input_ids, dtype=\"int64\")\r\n",
    "            # 累加输入,求出序列前K个的长度,比如[1,2,3,4]\r\n",
    "            seq_length = paddle.cumsum(ones, axis=-1)\r\n",
    "            # position id的形式： 比如[0,1,2,3]\r\n",
    "            position_ids = seq_length - ones\r\n",
    "            position_ids.stop_gradient = self.pos_stop_gradient\r\n",
    "        # 输入编码\r\n",
    "        input_embedings = self.word_embeddings(input_ids)\r\n",
    "        # 分段编码\r\n",
    "        segment_embeddings = self.segment_embeddings(segment_ids)\r\n",
    "        # 位置编码\r\n",
    "        position_embeddings = self.position_embeddings(position_ids)\r\n",
    "        # 输入张量, 分段张量，位置张量进行叠加\r\n",
    "        # print(input_embedings.shape, segment_embeddings.shape, position_embeddings.shape)\r\n",
    "        input_embedings = paddle.transpose(input_embedings, perm=[1, 0, 2])\r\n",
    "        position_embeddings = paddle.transpose(position_embeddings, perm=[1, 0, 2])\r\n",
    "        embeddings = input_embedings + segment_embeddings + position_embeddings\r\n",
    "        embeddings = paddle.transpose(embeddings, perm=[1, 0, 2])\r\n",
    "\r\n",
    "        # 层规范化\r\n",
    "        embeddings = self.layer_norm(embeddings)\r\n",
    "        # Dropout\r\n",
    "        embeddings = self.dropout(embeddings)\r\n",
    "        return embeddings\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class Model_Transformer_v1(nn.Layer):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        vocab_size,\r\n",
    "        n_block=1,\r\n",
    "        hidden_size=768,\r\n",
    "        heads_num=12,\r\n",
    "        intermediate_size=3072,\r\n",
    "        hidden_dropout=0.1,\r\n",
    "        attention_dropout=0.1,\r\n",
    "        act_dropout=0,\r\n",
    "        position_size=512,\r\n",
    "        num_classes=2,\r\n",
    "        padding_idx=0,\r\n",
    "        pos_stop_gradient=True,\r\n",
    "        random_pos=False\r\n",
    "    ):\r\n",
    "        super(Model_Transformer_v1, self).__init__()\r\n",
    "        # 词表大小\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        # Transformer的编码器的数目\r\n",
    "        self.n_block = n_block\r\n",
    "        # 每个词映射成稠密向量的维度\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        # 多头注意力的个数\r\n",
    "        self.heads_num = heads_num\r\n",
    "        # 逐位前馈层的的维度\r\n",
    "        self.intermediate_size = intermediate_size\r\n",
    "        # Embedding层的 Dropout\r\n",
    "        self.hidden_dropout = hidden_dropout\r\n",
    "        # 多头注意力的dropout的 dropout参数\r\n",
    "        self.attention_dropout = attention_dropout\r\n",
    "        # 位置编码的大小 position_size\r\n",
    "        self.position_size = position_size\r\n",
    "        # 类别数\r\n",
    "        self.num_classes = num_classes\r\n",
    "        # 逐位前馈层的dropout\r\n",
    "        self.act_dropout = act_dropout\r\n",
    "        # [PAD]字符的ID\r\n",
    "        self.padding_idx = padding_idx\r\n",
    "        # 实例化输入编码，分段编码和位置编码\r\n",
    "        self.embeddings = TransformerEmbeddings(\r\n",
    "            self.vocab_size, self.hidden_size, self.hidden_dropout, self.position_size, pos_stop_gradient=pos_stop_gradient, random_pos=random_pos)\r\n",
    "        # 实例化Transformer的编码器\r\n",
    "        self.layers = nn.LayerList([])\r\n",
    "        for i in range(n_block):\r\n",
    "            # 使用框架API\r\n",
    "            encoder_layer = nn.TransformerEncoderLayer(hidden_size, \r\n",
    "                                                    heads_num, \r\n",
    "                                                    intermediate_size,\r\n",
    "                                                    dropout=hidden_dropout,\r\n",
    "                                                    attn_dropout=attention_dropout,\r\n",
    "                                                    act_dropout=act_dropout)\r\n",
    "            self.layers.append(encoder_layer)\r\n",
    "        # 全连接层\r\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\r\n",
    "        # 双曲正切激活函数\r\n",
    "        self.activation = nn.Tanh()\r\n",
    "        # 最后一层分类器\r\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\r\n",
    "\r\n",
    "    def forward(self, inputs, position_ids=None, attention_mask=None):\r\n",
    "        input_ids, segment_ids = inputs\r\n",
    "        # 构建Mask矩阵，把Pad的位置即input_ids中为0的位置设置为True,非0的位置设置为False\r\n",
    "        if attention_mask is None:\r\n",
    "            attention_mask = paddle.unsqueeze(\r\n",
    "                (input_ids == self.padding_idx).astype(\"float32\") * -1e9, axis=[1, 2] )\r\n",
    "        # 抽取特征向量\r\n",
    "        embedding_output = self.embeddings(\r\n",
    "            input_ids=input_ids, position_ids=position_ids, segment_ids=segment_ids )\r\n",
    "        sequence_output = embedding_output\r\n",
    "        self._attention_weights = []\r\n",
    "        # Transformer的输出和注意力权重的输出\r\n",
    "        for i, encoder_layer in enumerate(self.layers):\r\n",
    "            sequence_output = encoder_layer(\r\n",
    "                sequence_output, src_mask=attention_mask )\r\n",
    "        # 选择第0个位置的向量作为句向量\r\n",
    "        first_token_tensor = sequence_output[:, 0]\r\n",
    "        # 输出层\r\n",
    "        pooled_output = self.dense(first_token_tensor)\r\n",
    "        pooled_output = self.activation(pooled_output)\r\n",
    "        # 句子级别的输出经过分类器\r\n",
    "        logits = self.classifier(pooled_output)\r\n",
    "        return logits\r\n",
    "\r\n",
    "paddle.seed(2021)\r\n",
    "heads_num = 4\r\n",
    "epochs = 3\r\n",
    "vocab_size=251890\r\n",
    "num_classes= 2\r\n",
    "# 注意力多头的数目\r\n",
    "# 交叉熵损失\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "# 评估的时候采用准确率指标\r\n",
    "metric = Accuracy()\r\n",
    "# Transformer的分类模型\r\n",
    "model1 = Model_Transformer_v1(\r\n",
    "    vocab_size=vocab_size,\r\n",
    "    n_block=1,\r\n",
    "    num_classes=num_classes,\r\n",
    "    heads_num=heads_num,\r\n",
    "    padding_idx=padding_idx,\r\n",
    "    pos_stop_gradient=False,\r\n",
    "    random_pos=True\r\n",
    ")\r\n",
    "# 排除所有的偏置和LayerNorm的参数\r\n",
    "decay_params1 = [\r\n",
    "    p.name for n, p in model1.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "# 定义 Optimizer\r\n",
    "optimizer1 = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=5e-5,\r\n",
    "    parameters=model1.parameters(),\r\n",
    "    weight_decay=0.0,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params1)\r\n",
    "\r\n",
    "runner1 = RunnerV3(model1, optimizer1, criterion, metric)\r\n",
    "save_path1=\"./checkpoint/model_best1.pdparams\"\r\n",
    "runner1.train(train_loader, dev_loader, num_epochs=epochs, log_steps=100, eval_steps=500, save_path=save_path1)\r\n",
    "\r\n",
    "# EVALUATE\r\n",
    "model_path1 = \"checkpoint/model_best1.pdparams\"\r\n",
    "runner1.load_model(model_path1)\r\n",
    "accuracy1, _ =  runner1.evaluate(test_loader)\r\n",
    "\r\n",
    "model2 = Model_Transformer_v1(\r\n",
    "    vocab_size=vocab_size,\r\n",
    "    n_block=2,\r\n",
    "    num_classes=num_classes,\r\n",
    "    heads_num=heads_num,\r\n",
    "    padding_idx=padding_idx,\r\n",
    "    pos_stop_gradient=False,\r\n",
    "    random_pos=True\r\n",
    ")\r\n",
    "# 排除所有的偏置和LayerNorm的参数\r\n",
    "decay_params2 = [\r\n",
    "    p.name for n, p in model2.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "# 定义 Optimizer\r\n",
    "optimizer2 = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=5e-5,\r\n",
    "    parameters=model2.parameters(),\r\n",
    "    weight_decay=0.0,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params2)\r\n",
    "\r\n",
    "runner2 = RunnerV3(model2, optimizer2, criterion, metric)\r\n",
    "save_path2=\"./checkpoint/model_best2.pdparams\"\r\n",
    "runner2.train(train_loader, dev_loader, num_epochs=epochs, log_steps=100, eval_steps=500, save_path=save_path2)\r\n",
    "\r\n",
    "model_path2 = \"checkpoint/model_best2.pdparams\"\r\n",
    "runner2.load_model(model_path2)\r\n",
    "accuracy2, _ =  runner2.evaluate(test_loader)\r\n",
    "\r\n",
    "print(f\"1-layer transformer model with learnable positional embedding: Accuracy: {accuracy1:.5f}\")\r\n",
    "print(f\"2-layer transformer model with learnable positional embedding: Accuracy: {accuracy2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以观察到，都采用可学习的位置编码时，双层的模型的精度（**0.80312**）显著高于单层模型的精度（**0.75352**）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.训练时候加入warmup的策略，并适当调整warmup的参数，并与不加warmup的实验进行对比。<span style=\"color:red\">(附加题&加分题)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/3, step: 0/588, loss: 0.80914\n",
      "[Train] epoch: 0/3, step: 100/588, loss: 0.63686\n",
      "[Train] epoch: 1/3, step: 200/588, loss: 0.50187\n",
      "[Train] epoch: 1/3, step: 300/588, loss: 0.28986\n",
      "[Train] epoch: 2/3, step: 400/588, loss: 0.24994\n",
      "[Train] epoch: 2/3, step: 500/588, loss: 0.11478\n",
      "[Evaluate]  dev score: 0.80160, dev loss: 0.54093\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.80160\n",
      "[Evaluate]  dev score: 0.80904, dev loss: 0.55051\n",
      "[Evaluate] best accuracy performence has been updated: 0.80160 --> 0.80904\n",
      "[Train] Training done!\n",
      "2-layer transformer model with learnable positional embedding and linear warm-up scheduler \n",
      " : Accuracy: 0.80408\n"
     ]
    }
   ],
   "source": [
    "model3 = Model_Transformer_v1(\r\n",
    "    vocab_size=vocab_size,\r\n",
    "    n_block=2,\r\n",
    "    num_classes=num_classes,\r\n",
    "    heads_num=heads_num,\r\n",
    "    padding_idx=padding_idx,\r\n",
    "    pos_stop_gradient=False,\r\n",
    "    random_pos=True\r\n",
    ")\r\n",
    "# 排除所有的偏置和LayerNorm的参数\r\n",
    "decay_params3 = [\r\n",
    "    p.name for n, p in model3.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "\r\n",
    "# 定义 Optimizer\r\n",
    "linear_warmup_scheduler = paddle.optimizer.lr.LinearWarmup(\r\n",
    "        learning_rate=5e-5, warmup_steps=3, start_lr=0, end_lr=5e-5, verbose=False)\r\n",
    "optimizer3 = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=linear_warmup_scheduler,\r\n",
    "    parameters=model3.parameters(),\r\n",
    "    weight_decay=0.0,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params3)\r\n",
    "\r\n",
    "runner3 = RunnerV3(model3, optimizer3, criterion, metric)\r\n",
    "save_path3=\"./checkpoint/model_best3.pdparams\"\r\n",
    "runner3.train(train_loader, dev_loader, num_epochs=epochs, log_steps=100, eval_steps=500, save_path=save_path3, scheduler=linear_warmup_scheduler)\r\n",
    "\r\n",
    "model_path3 = \"checkpoint/model_best3.pdparams\"\r\n",
    "runner3.load_model(model_path3)\r\n",
    "accuracy3, _ =  runner3.evaluate(test_loader)\r\n",
    "print(f\"2-layer transformer model with learnable positional embedding and linear warm-up scheduler \\n : Accuracy: {accuracy3:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "适当调整线性学习率热身的参数之后，2层的带可学习位置编码的模型在测试集上的准确率从**0.80312**提高到了**0.80408**，提升不显著。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. <span style=\"color:red\">(附加题&简答题&加分题)</span>\n",
    "\n",
    "    小明是一位建筑设计师，在工作中经常需要手工的查询一些建筑规范条文，规范条文非常的多，每次都需要手工翻阅那些规范条文，非常的不方便。小李是一位计算机的学生，当得知小明的情况后，想用技术手段帮助小明，最开始使用基于关键词的匹配，但是对字面上相同，语义不同和字面上不一样，但语义相似的情况不好处理，于是想利用神经网络的方法来解决这个问题，小李构建了双向LSTM的网络，并在LSTM上加了点积注意力，对于一个小批次的数据不同长度的句子，加入对齐字符进行了对齐，在计算注意力机制的时候，对于一些对齐的字符也计算注意力，并分配注意力权重，最后将训练好的网络融入到规范条文匹配中，请问上述的做法有什么问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "问题在于**将用于补齐的字符``[pad]``直接纳入到了注意力机制的计算过程中**。\n",
    "\n",
    "应当设置mask矩阵在``[pad]``对应的位置上赋给很大的负值，这样在得到的注意力分布中，对齐字符的权重接近于0，才能减少特殊字符对注意力机制的影响。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
