{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 第六章 循环神经网络\n",
    "\n",
    "### 1.本节的IMDB电影评论文本分类任务中，默认使用了单层的Paddle LSTM模型，请尝试使用叠加多层进行训练，观察其效果并与单层LSTM进行对比。<span style=\"color:red\">(必修题)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"the premise of an african-american female scrooge in the modern, struggling city was inspired, but nothing else in this film is. here, ms. scrooge is a miserly banker who takes advantage of the employees and customers in the largely poor and black neighborhood it inhabits. there is no doubt about the good intentions of the people involved. part of the problem is that story's roots don't translate well into the urban setting of this film, and the script fails to make the update work. also, the constant message about sharing and giving is repeated so endlessly, the audience becomes tired of it well before the movie reaches its familiar end. this is a message film that doesn't know when to quit. in the title role, the talented cicely tyson gives an overly uptight performance, and at times lines are difficult to understand. the charles dickens novel has been adapted so many times, it's a struggle to adapt it in a way that makes it fresh and relevant, in spite of its very relevant message.\", '0')\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "# 加载数据集\r\n",
    "def load_imdb_data(path):\r\n",
    "    assert os.path.exists(path) \r\n",
    "    trainset, devset, testset = [], [], []\r\n",
    "    with open(os.path.join(path, \"train.txt\"), \"r\") as fr:\r\n",
    "        for line in fr:\r\n",
    "            sentence_label, sentence = line.strip().lower().split(\"\\t\", maxsplit=1)\r\n",
    "            trainset.append((sentence, sentence_label))\r\n",
    "\r\n",
    "    with open(os.path.join(path, \"dev.txt\"), \"r\") as fr:\r\n",
    "        for line in fr:\r\n",
    "            sentence_label, sentence = line.strip().lower().split(\"\\t\", maxsplit=1)\r\n",
    "            devset.append((sentence, sentence_label))\r\n",
    "\r\n",
    "    with open(os.path.join(path, \"test.txt\"), \"r\") as fr:\r\n",
    "        for line in fr:\r\n",
    "            sentence_label, sentence = line.strip().lower().split(\"\\t\", maxsplit=1)\r\n",
    "            testset.append((sentence, sentence_label))\r\n",
    "\r\n",
    "    return trainset, devset, testset\r\n",
    "\r\n",
    "# 加载IMDB数据集\r\n",
    "train_data, dev_data, test_data = load_imdb_data(\"./dataset/\") \r\n",
    "# 打印一下加载后的数据样式\r\n",
    "print(train_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数： 25000\n",
      "样本示例： ([2, 976, 5, 32, 6860, 618, 7673, 8, 2, 13073, 2525, 724, 14, 22837, 18, 164, 416, 8, 10, 24, 701, 611, 1743, 7673, 7, 3, 56391, 21652, 36, 271, 3495, 5, 2, 11373, 4, 13244, 8, 2, 2157, 350, 4, 328, 4118, 12, 48810, 52, 7, 60, 860, 43, 2, 56, 4393, 5, 2, 89, 4152, 182, 5, 2, 461, 7, 11, 7321, 7730, 86, 7931, 107, 72, 2, 2830, 1165, 5, 10, 151, 4, 2, 272, 1003, 6, 91, 2, 10491, 912, 826, 2, 1750, 889, 43, 6723, 4, 647, 7, 2535, 38, 39222, 2, 357, 398, 1505, 5, 12, 107, 179, 2, 20, 4279, 83, 1163, 692, 10, 7, 3, 889, 24, 11, 141, 118, 50, 6, 28642, 8, 2, 490, 1469, 2, 1039, 98975, 24541, 344, 32, 2074, 11852, 1683, 4, 29, 286, 478, 22, 823, 6, 5222, 2, 1490, 6893, 883, 41, 71, 3254, 38, 100, 1021, 44, 3, 1700, 6, 8768, 12, 8, 3, 108, 11, 146, 12, 1761, 4, 92295, 8, 2641, 5, 83, 49, 3866, 5352], 0)\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "from paddle.io import Dataset\r\n",
    "from utils.data import load_vocab\r\n",
    "\r\n",
    "class IMDBDataset(Dataset):\r\n",
    "    def __init__(self, examples, word2id_dict):\r\n",
    "        super(IMDBDataset, self).__init__()\r\n",
    "        # 词典，用于将单词转为字典索引的数字\r\n",
    "        self.word2id_dict =  word2id_dict\r\n",
    "        # 加载后的数据集\r\n",
    "        self.examples = self.words_to_id(examples)\r\n",
    "\r\n",
    "    def words_to_id(self, examples):\r\n",
    "        tmp_examples = []\r\n",
    "        for idx, example in enumerate(examples):\r\n",
    "            seq, label = example\r\n",
    "            # 将单词映射为字典索引的ID， 对于词典中没有的单词用[UNK]对应的ID进行替代\r\n",
    "            seq = [self.word2id_dict.get(word, self.word2id_dict['[UNK]']) for word in seq.split(\" \")]\r\n",
    "            label = int(label)\r\n",
    "            tmp_examples.append([seq, label])\r\n",
    "        return tmp_examples\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        seq, label = self.examples[idx]\r\n",
    "        return seq, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.examples)\r\n",
    "    \r\n",
    "# 加载词表\r\n",
    "word2id_dict= load_vocab(\"./dataset/vocab.txt\") \r\n",
    "\r\n",
    "# 实例化Dataset\r\n",
    "train_set = IMDBDataset(train_data, word2id_dict)\r\n",
    "dev_set = IMDBDataset(dev_data, word2id_dict)\r\n",
    "test_set = IMDBDataset(test_data, word2id_dict)\r\n",
    "\r\n",
    "print('训练集样本数：', len(train_set))\r\n",
    "print('样本示例：', train_set[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\r\n",
    "\r\n",
    "def collate_fn(batch_data, pad_val=0, max_seq_len=256):\r\n",
    "    seqs, seq_lens, labels = [], [], []\r\n",
    "    max_len = 0\r\n",
    "    for example in batch_data:\r\n",
    "        seq, label = example\r\n",
    "        # 对数据序列进行截断\r\n",
    "        seq = seq[:max_seq_len]\r\n",
    "        # 对数据截断并保存于seqs中\r\n",
    "        seqs.append(seq)\r\n",
    "        seq_lens.append(len(seq))\r\n",
    "        labels.append(label)\r\n",
    "        # 保存序列最大长度\r\n",
    "        max_len = max(max_len, len(seq))\r\n",
    "    # 对数据序列进行填充至最大长度\r\n",
    "    for i in range(len(seqs)):\r\n",
    "        seqs[i] = seqs[i] + [pad_val] * (max_len - len(seqs[i]))\r\n",
    "\r\n",
    "    return (paddle.to_tensor(seqs), paddle.to_tensor(seq_lens)), paddle.to_tensor(labels)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqs:  Tensor(shape=[2, 5], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[1, 2, 3, 4, 5],\n",
      "        [2, 4, 6, 0, 0]])\n",
      "seq_lens:  Tensor(shape=[2], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [5, 3])\n",
      "labels:  Tensor(shape=[2], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [1, 0])\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 5\r\n",
    "batch_data = [[[1, 2, 3, 4, 5, 6], 1], [[2,4,6], 0]]\r\n",
    "(seqs, seq_lens), labels = collate_fn(batch_data, pad_val=word2id_dict[\"[PAD]\"], max_seq_len=max_seq_len)\r\n",
    "print(\"seqs: \", seqs)\r\n",
    "print(\"seq_lens: \", seq_lens)\r\n",
    "print(\"labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_seq_len = 256\r\n",
    "batch_size = 128\r\n",
    "collate_fn = partial(collate_fn, pad_val=word2id_dict[\"[PAD]\"], max_seq_len=max_seq_len)\r\n",
    "train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=collate_fn)\r\n",
    "dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn)\r\n",
    "test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AveragePooling(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(AveragePooling, self).__init__()\r\n",
    "    \r\n",
    "    def forward(self, sequence_output, sequence_length):\r\n",
    "        sequence_length = paddle.cast(sequence_length.unsqueeze(-1), dtype=\"float32\")\r\n",
    "        # 根据sequence_length生成mask矩阵，用于对Padding位置的信息进行mask\r\n",
    "        max_len = sequence_output.shape[1]\r\n",
    "        mask = paddle.arange(max_len) < sequence_length\r\n",
    "        mask = paddle.cast(mask, dtype=\"float32\").unsqueeze(-1)\r\n",
    "        # 对序列中paddling部分进行mask\r\n",
    "        sequence_output = paddle.multiply(sequence_output, mask)\r\n",
    "        # 对序列中的向量取均值\r\n",
    "        batch_mean_hidden = paddle.divide(paddle.sum(sequence_output, axis=1), sequence_length)\r\n",
    "        return batch_mean_hidden\r\n",
    "\r\n",
    "class Model_BiLSTM_FC(nn.Layer):\r\n",
    "    def __init__(self, num_embeddings, input_size, hidden_size, num_classes=2):\r\n",
    "        super(Model_BiLSTM_FC, self).__init__()\r\n",
    "        # 词典大小\r\n",
    "        self.num_embeddings = num_embeddings\r\n",
    "        # 单词向量的维度\r\n",
    "        self.input_size = input_size\r\n",
    "        # LSTM隐藏单元数量\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        # 情感分类类别数量\r\n",
    "        self.num_classes = num_classes\r\n",
    "        # 实例化嵌入层\r\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings, input_size, padding_idx=0)\r\n",
    "        # 实例化LSTM层\r\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, direction=\"forward\")\r\n",
    "        # 实例化聚合层\r\n",
    "        self.average_layer = AveragePooling()\r\n",
    "        # 实例化输出层\r\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        # 对模型输入拆分为序列数据和mask\r\n",
    "        input_ids, sequence_length = inputs\r\n",
    "        # 获取词向量\r\n",
    "        inputs_emb = self.embedding_layer(input_ids)\r\n",
    "        # 使用lstm处理数据\r\n",
    "        sequence_output, _ = self.lstm_layer(inputs_emb, sequence_length=sequence_length)\r\n",
    "        # 使用聚合层聚合sequence_output\r\n",
    "        batch_mean_hidden = self.average_layer(sequence_output, sequence_length)\r\n",
    "        # 输出文本分类logits\r\n",
    "        logits = self.output_layer(batch_mean_hidden)\r\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "W0727 00:57:33.701588  9852 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0727 00:57:33.704583  9852 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:278: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.int64, but right dtype is paddle.float32, the right dtype will convert to paddle.int64\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/3, step: 0/588, loss: 0.69244\n",
      "[Train] epoch: 0/3, step: 10/588, loss: 0.68635\n",
      "[Evaluate]  dev score: 0.53960, dev loss: 0.68583\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.53960\n",
      "[Train] epoch: 0/3, step: 20/588, loss: 0.64868\n",
      "[Evaluate]  dev score: 0.64344, dev loss: 0.63686\n",
      "[Evaluate] best accuracy performence has been updated: 0.53960 --> 0.64344\n",
      "[Train] epoch: 0/3, step: 30/588, loss: 0.52741\n",
      "[Evaluate]  dev score: 0.75936, dev loss: 0.55803\n",
      "[Evaluate] best accuracy performence has been updated: 0.64344 --> 0.75936\n",
      "[Train] epoch: 0/3, step: 40/588, loss: 0.50660\n",
      "[Evaluate]  dev score: 0.79368, dev loss: 0.52721\n",
      "[Evaluate] best accuracy performence has been updated: 0.75936 --> 0.79368\n",
      "[Train] epoch: 0/3, step: 50/588, loss: 0.40474\n",
      "[Evaluate]  dev score: 0.80776, dev loss: 0.42634\n",
      "[Evaluate] best accuracy performence has been updated: 0.79368 --> 0.80776\n",
      "[Train] epoch: 0/3, step: 60/588, loss: 0.30223\n",
      "[Evaluate]  dev score: 0.80560, dev loss: 0.44791\n",
      "[Train] epoch: 0/3, step: 70/588, loss: 0.40518\n",
      "[Evaluate]  dev score: 0.84296, dev loss: 0.37650\n",
      "[Evaluate] best accuracy performence has been updated: 0.80776 --> 0.84296\n",
      "[Train] epoch: 0/3, step: 80/588, loss: 0.29521\n",
      "[Evaluate]  dev score: 0.84864, dev loss: 0.35303\n",
      "[Evaluate] best accuracy performence has been updated: 0.84296 --> 0.84864\n",
      "[Train] epoch: 0/3, step: 90/588, loss: 0.28628\n",
      "[Evaluate]  dev score: 0.84560, dev loss: 0.35342\n",
      "[Train] epoch: 0/3, step: 100/588, loss: 0.36619\n",
      "[Evaluate]  dev score: 0.85408, dev loss: 0.34148\n",
      "[Evaluate] best accuracy performence has been updated: 0.84864 --> 0.85408\n",
      "[Train] epoch: 0/3, step: 110/588, loss: 0.30730\n",
      "[Evaluate]  dev score: 0.85472, dev loss: 0.33962\n",
      "[Evaluate] best accuracy performence has been updated: 0.85408 --> 0.85472\n",
      "[Train] epoch: 0/3, step: 120/588, loss: 0.29427\n",
      "[Evaluate]  dev score: 0.85128, dev loss: 0.34943\n",
      "[Train] epoch: 0/3, step: 130/588, loss: 0.24082\n",
      "[Evaluate]  dev score: 0.85656, dev loss: 0.33489\n",
      "[Evaluate] best accuracy performence has been updated: 0.85472 --> 0.85656\n",
      "[Train] epoch: 0/3, step: 140/588, loss: 0.23754\n",
      "[Evaluate]  dev score: 0.85072, dev loss: 0.34151\n",
      "[Train] epoch: 0/3, step: 150/588, loss: 0.40865\n",
      "[Evaluate]  dev score: 0.84344, dev loss: 0.36174\n",
      "[Train] epoch: 0/3, step: 160/588, loss: 0.26130\n",
      "[Evaluate]  dev score: 0.83800, dev loss: 0.36801\n",
      "[Train] epoch: 0/3, step: 170/588, loss: 0.30863\n",
      "[Evaluate]  dev score: 0.85984, dev loss: 0.33011\n",
      "[Evaluate] best accuracy performence has been updated: 0.85656 --> 0.85984\n",
      "[Train] epoch: 0/3, step: 180/588, loss: 0.26716\n",
      "[Evaluate]  dev score: 0.85952, dev loss: 0.32840\n",
      "[Train] epoch: 0/3, step: 190/588, loss: 0.22063\n",
      "[Evaluate]  dev score: 0.85248, dev loss: 0.34317\n",
      "[Train] epoch: 1/3, step: 200/588, loss: 0.11712\n",
      "[Evaluate]  dev score: 0.85776, dev loss: 0.33348\n",
      "[Train] epoch: 1/3, step: 210/588, loss: 0.13256\n",
      "[Evaluate]  dev score: 0.85480, dev loss: 0.42600\n",
      "[Train] epoch: 1/3, step: 220/588, loss: 0.08873\n",
      "[Evaluate]  dev score: 0.85464, dev loss: 0.36987\n",
      "[Train] epoch: 1/3, step: 230/588, loss: 0.06369\n",
      "[Evaluate]  dev score: 0.84832, dev loss: 0.42533\n",
      "[Train] epoch: 1/3, step: 240/588, loss: 0.10127\n",
      "[Evaluate]  dev score: 0.85040, dev loss: 0.45054\n",
      "[Train] epoch: 1/3, step: 250/588, loss: 0.02441\n",
      "[Evaluate]  dev score: 0.84512, dev loss: 0.38362\n",
      "[Train] epoch: 1/3, step: 260/588, loss: 0.14579\n",
      "[Evaluate]  dev score: 0.83696, dev loss: 0.45806\n",
      "[Train] epoch: 1/3, step: 270/588, loss: 0.05579\n",
      "[Evaluate]  dev score: 0.83952, dev loss: 0.44039\n",
      "[Train] epoch: 1/3, step: 280/588, loss: 0.10467\n",
      "[Evaluate]  dev score: 0.84104, dev loss: 0.37858\n",
      "[Train] epoch: 1/3, step: 290/588, loss: 0.06398\n",
      "[Evaluate]  dev score: 0.84448, dev loss: 0.38734\n",
      "[Train] epoch: 1/3, step: 300/588, loss: 0.07188\n",
      "[Evaluate]  dev score: 0.84008, dev loss: 0.49121\n",
      "[Train] epoch: 1/3, step: 310/588, loss: 0.04940\n",
      "[Evaluate]  dev score: 0.84256, dev loss: 0.43231\n",
      "[Train] epoch: 1/3, step: 320/588, loss: 0.08648\n",
      "[Evaluate]  dev score: 0.83424, dev loss: 0.45224\n",
      "[Train] epoch: 1/3, step: 330/588, loss: 0.05941\n",
      "[Evaluate]  dev score: 0.83552, dev loss: 0.42183\n",
      "[Train] epoch: 1/3, step: 340/588, loss: 0.11060\n",
      "[Evaluate]  dev score: 0.83512, dev loss: 0.47309\n",
      "[Train] epoch: 1/3, step: 350/588, loss: 0.07351\n",
      "[Evaluate]  dev score: 0.82800, dev loss: 0.49048\n",
      "[Train] epoch: 1/3, step: 360/588, loss: 0.13024\n",
      "[Evaluate]  dev score: 0.82632, dev loss: 0.42336\n",
      "[Train] epoch: 1/3, step: 370/588, loss: 0.07413\n",
      "[Evaluate]  dev score: 0.83176, dev loss: 0.46987\n",
      "[Train] epoch: 1/3, step: 380/588, loss: 0.05594\n",
      "[Evaluate]  dev score: 0.83608, dev loss: 0.43618\n",
      "[Train] epoch: 1/3, step: 390/588, loss: 0.09798\n",
      "[Evaluate]  dev score: 0.83416, dev loss: 0.42642\n",
      "[Train] epoch: 2/3, step: 400/588, loss: 0.00286\n",
      "[Evaluate]  dev score: 0.82664, dev loss: 0.63097\n",
      "[Train] epoch: 2/3, step: 410/588, loss: 0.00178\n",
      "[Evaluate]  dev score: 0.83016, dev loss: 0.82318\n",
      "[Train] epoch: 2/3, step: 420/588, loss: 0.01482\n",
      "[Evaluate]  dev score: 0.82224, dev loss: 0.62087\n",
      "[Train] epoch: 2/3, step: 430/588, loss: 0.01209\n",
      "[Evaluate]  dev score: 0.82648, dev loss: 0.63014\n",
      "[Train] epoch: 2/3, step: 440/588, loss: 0.01738\n",
      "[Evaluate]  dev score: 0.78856, dev loss: 1.08283\n",
      "[Train] epoch: 2/3, step: 450/588, loss: 0.03365\n",
      "[Evaluate]  dev score: 0.79216, dev loss: 0.61345\n",
      "[Train] epoch: 2/3, step: 460/588, loss: 0.03863\n",
      "[Evaluate]  dev score: 0.83176, dev loss: 0.51473\n",
      "[Train] epoch: 2/3, step: 470/588, loss: 0.06770\n",
      "[Evaluate]  dev score: 0.83296, dev loss: 0.64559\n",
      "[Train] epoch: 2/3, step: 480/588, loss: 0.01363\n",
      "[Evaluate]  dev score: 0.82600, dev loss: 0.71234\n",
      "[Train] epoch: 2/3, step: 490/588, loss: 0.01066\n",
      "[Evaluate]  dev score: 0.81368, dev loss: 0.83082\n",
      "[Train] epoch: 2/3, step: 500/588, loss: 0.00620\n",
      "[Evaluate]  dev score: 0.81424, dev loss: 0.67613\n",
      "[Train] epoch: 2/3, step: 510/588, loss: 0.03387\n",
      "[Evaluate]  dev score: 0.83472, dev loss: 0.47519\n",
      "[Train] epoch: 2/3, step: 520/588, loss: 0.03234\n",
      "[Evaluate]  dev score: 0.81760, dev loss: 0.60178\n",
      "[Train] epoch: 2/3, step: 530/588, loss: 0.00786\n",
      "[Evaluate]  dev score: 0.79976, dev loss: 0.81250\n",
      "[Train] epoch: 2/3, step: 540/588, loss: 0.01410\n",
      "[Evaluate]  dev score: 0.83496, dev loss: 0.50207\n",
      "[Train] epoch: 2/3, step: 550/588, loss: 0.02234\n",
      "[Evaluate]  dev score: 0.81984, dev loss: 0.63174\n",
      "[Train] epoch: 2/3, step: 560/588, loss: 0.01913\n",
      "[Evaluate]  dev score: 0.83376, dev loss: 0.62241\n",
      "[Train] epoch: 2/3, step: 570/588, loss: 0.00193\n",
      "[Evaluate]  dev score: 0.82400, dev loss: 0.72101\n",
      "[Train] epoch: 2/3, step: 580/588, loss: 0.02221\n",
      "[Evaluate]  dev score: 0.81800, dev loss: 0.60016\n",
      "[Evaluate]  dev score: 0.83296, dev loss: 0.53822\n",
      "[Train] Training done!\n",
      "time:  64.83894395828247\n"
     ]
    }
   ],
   "source": [
    "import time\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "from nndl import Accuracy, RunnerV3\r\n",
    "\r\n",
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "paddle.seed(0)\r\n",
    "\r\n",
    "# 指定训练轮次\r\n",
    "num_epochs = 3\r\n",
    "# 指定学习率\r\n",
    "learning_rate = 0.001\r\n",
    "# 指定embedding的数量为词表长度\r\n",
    "num_embeddings = len(word2id_dict)\r\n",
    "# embedding向量的维度\r\n",
    "input_size = 256\r\n",
    "# LSTM网络隐状态向量的维度\r\n",
    "hidden_size = 256\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = Model_BiLSTM_FC(num_embeddings, input_size, hidden_size)\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= model.parameters()) \r\n",
    "# 指定损失函数\r\n",
    "loss_fn = paddle.nn.CrossEntropyLoss() \r\n",
    "# 指定评估指标\r\n",
    "metric = Accuracy()\r\n",
    "# 实例化Runner\r\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\r\n",
    "# 模型训练\r\n",
    "start_time = time.time()\r\n",
    "runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=10, log_steps=10, save_path=\"./checkpoints/best_forward.pdparams\")\r\n",
    "end_time = time.time()\r\n",
    "print(\"time: \", (end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1492: UserWarning: Skip loading for lstm_layer.weight_ih_l0. lstm_layer.weight_ih_l0 is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1492: UserWarning: Skip loading for lstm_layer.weight_hh_l0. lstm_layer.weight_hh_l0 is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1492: UserWarning: Skip loading for lstm_layer.bias_ih_l0. lstm_layer.bias_ih_l0 is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1492: UserWarning: Skip loading for lstm_layer.bias_hh_l0. lstm_layer.bias_hh_l0 is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1492: UserWarning: Skip loading for lstm_layer.0.cell.weight_ih. lstm_layer.0.cell.weight_ih is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1492: UserWarning: Skip loading for lstm_layer.0.cell.weight_hh. lstm_layer.0.cell.weight_hh is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1492: UserWarning: Skip loading for lstm_layer.0.cell.bias_ih. lstm_layer.0.cell.bias_ih is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1492: UserWarning: Skip loading for lstm_layer.0.cell.bias_hh. lstm_layer.0.cell.bias_hh is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set, Accuracy: 0.82880\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./checkpoints/best_self_forward.pdparams\"\r\n",
    "runner.load_model(model_path)\r\n",
    "accuracy, _ =  runner.evaluate(test_loader)\r\n",
    "print(f\"Evaluate on test set, Accuracy: {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model_MultiLayer_BiLSTM_FC(nn.Layer):\r\n",
    "    def __init__(self, num_embeddings, input_size, hidden_size, num_classes=2):\r\n",
    "        super(Model_MultiLayer_BiLSTM_FC, self).__init__()\r\n",
    "        # 词典大小\r\n",
    "        self.num_embeddings = num_embeddings\r\n",
    "        # 单词向量的维度\r\n",
    "        self.input_size = input_size\r\n",
    "        # LSTM隐藏单元数量\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        # 情感分类类别数量\r\n",
    "        self.num_classes = num_classes\r\n",
    "        # 实例化嵌入层\r\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings, input_size, padding_idx=0)\r\n",
    "        # 实例化LSTM层\r\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, num_layers=2, direction=\"forward\")\r\n",
    "        # 实例化聚合层\r\n",
    "        self.average_layer = AveragePooling()\r\n",
    "        # 实例化输出层\r\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        # 对模型输入拆分为序列数据和mask\r\n",
    "        input_ids, sequence_length = inputs\r\n",
    "        # 获取词向量\r\n",
    "        inputs_emb = self.embedding_layer(input_ids)\r\n",
    "        # 使用lstm处理数据\r\n",
    "        sequence_output, _ = self.lstm_layer(inputs_emb, sequence_length=sequence_length)\r\n",
    "        # 使用聚合层聚合sequence_output\r\n",
    "        batch_mean_hidden = self.average_layer(sequence_output, sequence_length)\r\n",
    "        # 输出文本分类logits\r\n",
    "        logits = self.output_layer(batch_mean_hidden)\r\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/3, step: 0/588, loss: 0.69207\n",
      "[Train] epoch: 0/3, step: 10/588, loss: 0.69213\n",
      "[Evaluate]  dev score: 0.49472, dev loss: 0.69272\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.49472\n",
      "[Train] epoch: 0/3, step: 20/588, loss: 0.68635\n",
      "[Evaluate]  dev score: 0.50888, dev loss: 0.68027\n",
      "[Evaluate] best accuracy performence has been updated: 0.49472 --> 0.50888\n",
      "[Train] epoch: 0/3, step: 30/588, loss: 0.44815\n",
      "[Evaluate]  dev score: 0.75944, dev loss: 0.51048\n",
      "[Evaluate] best accuracy performence has been updated: 0.50888 --> 0.75944\n",
      "[Train] epoch: 0/3, step: 40/588, loss: 0.30420\n",
      "[Evaluate]  dev score: 0.79784, dev loss: 0.46734\n",
      "[Evaluate] best accuracy performence has been updated: 0.75944 --> 0.79784\n",
      "[Train] epoch: 0/3, step: 50/588, loss: 0.35504\n",
      "[Evaluate]  dev score: 0.78024, dev loss: 0.45525\n",
      "[Train] epoch: 0/3, step: 60/588, loss: 0.27823\n",
      "[Evaluate]  dev score: 0.83696, dev loss: 0.37768\n",
      "[Evaluate] best accuracy performence has been updated: 0.79784 --> 0.83696\n",
      "[Train] epoch: 0/3, step: 70/588, loss: 0.39443\n",
      "[Evaluate]  dev score: 0.84104, dev loss: 0.36397\n",
      "[Evaluate] best accuracy performence has been updated: 0.83696 --> 0.84104\n",
      "[Train] epoch: 0/3, step: 80/588, loss: 0.26798\n",
      "[Evaluate]  dev score: 0.84336, dev loss: 0.36206\n",
      "[Evaluate] best accuracy performence has been updated: 0.84104 --> 0.84336\n",
      "[Train] epoch: 0/3, step: 90/588, loss: 0.30705\n",
      "[Evaluate]  dev score: 0.84976, dev loss: 0.35033\n",
      "[Evaluate] best accuracy performence has been updated: 0.84336 --> 0.84976\n",
      "[Train] epoch: 0/3, step: 100/588, loss: 0.36560\n",
      "[Evaluate]  dev score: 0.85040, dev loss: 0.34779\n",
      "[Evaluate] best accuracy performence has been updated: 0.84976 --> 0.85040\n",
      "[Train] epoch: 0/3, step: 110/588, loss: 0.29950\n",
      "[Evaluate]  dev score: 0.85272, dev loss: 0.34586\n",
      "[Evaluate] best accuracy performence has been updated: 0.85040 --> 0.85272\n",
      "[Train] epoch: 0/3, step: 120/588, loss: 0.33002\n",
      "[Evaluate]  dev score: 0.84616, dev loss: 0.35933\n",
      "[Train] epoch: 0/3, step: 130/588, loss: 0.25191\n",
      "[Evaluate]  dev score: 0.85656, dev loss: 0.33837\n",
      "[Evaluate] best accuracy performence has been updated: 0.85272 --> 0.85656\n",
      "[Train] epoch: 0/3, step: 140/588, loss: 0.25718\n",
      "[Evaluate]  dev score: 0.85200, dev loss: 0.34135\n",
      "[Train] epoch: 0/3, step: 150/588, loss: 0.40654\n",
      "[Evaluate]  dev score: 0.84264, dev loss: 0.35763\n",
      "[Train] epoch: 0/3, step: 160/588, loss: 0.25887\n",
      "[Evaluate]  dev score: 0.82904, dev loss: 0.37886\n",
      "[Train] epoch: 0/3, step: 170/588, loss: 0.28161\n",
      "[Evaluate]  dev score: 0.84280, dev loss: 0.36315\n",
      "[Train] epoch: 0/3, step: 180/588, loss: 0.28110\n",
      "[Evaluate]  dev score: 0.85504, dev loss: 0.34110\n",
      "[Train] epoch: 0/3, step: 190/588, loss: 0.18416\n",
      "[Evaluate]  dev score: 0.86008, dev loss: 0.33098\n",
      "[Evaluate] best accuracy performence has been updated: 0.85656 --> 0.86008\n",
      "[Train] epoch: 1/3, step: 200/588, loss: 0.13088\n",
      "[Evaluate]  dev score: 0.85936, dev loss: 0.33031\n",
      "[Train] epoch: 1/3, step: 210/588, loss: 0.11675\n",
      "[Evaluate]  dev score: 0.85168, dev loss: 0.45852\n",
      "[Train] epoch: 1/3, step: 220/588, loss: 0.09244\n",
      "[Evaluate]  dev score: 0.85280, dev loss: 0.37656\n",
      "[Train] epoch: 1/3, step: 230/588, loss: 0.05074\n",
      "[Evaluate]  dev score: 0.84432, dev loss: 0.43434\n",
      "[Train] epoch: 1/3, step: 240/588, loss: 0.10061\n",
      "[Evaluate]  dev score: 0.84136, dev loss: 0.51181\n",
      "[Train] epoch: 1/3, step: 250/588, loss: 0.01657\n",
      "[Evaluate]  dev score: 0.84816, dev loss: 0.39937\n",
      "[Train] epoch: 1/3, step: 260/588, loss: 0.15038\n",
      "[Evaluate]  dev score: 0.84400, dev loss: 0.40216\n",
      "[Train] epoch: 1/3, step: 270/588, loss: 0.06123\n",
      "[Evaluate]  dev score: 0.84552, dev loss: 0.44253\n",
      "[Train] epoch: 1/3, step: 280/588, loss: 0.08976\n",
      "[Evaluate]  dev score: 0.84408, dev loss: 0.39538\n",
      "[Train] epoch: 1/3, step: 290/588, loss: 0.05078\n",
      "[Evaluate]  dev score: 0.84416, dev loss: 0.39388\n",
      "[Train] epoch: 1/3, step: 300/588, loss: 0.08405\n",
      "[Evaluate]  dev score: 0.83928, dev loss: 0.52228\n",
      "[Train] epoch: 1/3, step: 310/588, loss: 0.05126\n",
      "[Evaluate]  dev score: 0.84192, dev loss: 0.47528\n",
      "[Train] epoch: 1/3, step: 320/588, loss: 0.07736\n",
      "[Evaluate]  dev score: 0.83280, dev loss: 0.45623\n",
      "[Train] epoch: 1/3, step: 330/588, loss: 0.05064\n",
      "[Evaluate]  dev score: 0.83496, dev loss: 0.41921\n",
      "[Train] epoch: 1/3, step: 340/588, loss: 0.10670\n",
      "[Evaluate]  dev score: 0.83424, dev loss: 0.49209\n",
      "[Train] epoch: 1/3, step: 350/588, loss: 0.05414\n",
      "[Evaluate]  dev score: 0.82528, dev loss: 0.48341\n",
      "[Train] epoch: 1/3, step: 360/588, loss: 0.09269\n",
      "[Evaluate]  dev score: 0.82760, dev loss: 0.46922\n",
      "[Train] epoch: 1/3, step: 370/588, loss: 0.07062\n",
      "[Evaluate]  dev score: 0.83360, dev loss: 0.49927\n",
      "[Train] epoch: 1/3, step: 380/588, loss: 0.05582\n",
      "[Evaluate]  dev score: 0.83488, dev loss: 0.42578\n",
      "[Train] epoch: 1/3, step: 390/588, loss: 0.11711\n",
      "[Evaluate]  dev score: 0.82648, dev loss: 0.46600\n",
      "[Train] epoch: 2/3, step: 400/588, loss: 0.00352\n",
      "[Evaluate]  dev score: 0.82976, dev loss: 0.58871\n",
      "[Train] epoch: 2/3, step: 410/588, loss: 0.00404\n",
      "[Evaluate]  dev score: 0.79752, dev loss: 1.05704\n",
      "[Train] epoch: 2/3, step: 420/588, loss: 0.00376\n",
      "[Evaluate]  dev score: 0.82976, dev loss: 0.71615\n",
      "[Train] epoch: 2/3, step: 430/588, loss: 0.02180\n",
      "[Evaluate]  dev score: 0.81160, dev loss: 0.74421\n",
      "[Train] epoch: 2/3, step: 440/588, loss: 0.01167\n",
      "[Evaluate]  dev score: 0.80104, dev loss: 0.81167\n",
      "[Train] epoch: 2/3, step: 450/588, loss: 0.01097\n",
      "[Evaluate]  dev score: 0.83408, dev loss: 0.58704\n",
      "[Train] epoch: 2/3, step: 460/588, loss: 0.02728\n",
      "[Evaluate]  dev score: 0.82824, dev loss: 0.69029\n",
      "[Train] epoch: 2/3, step: 470/588, loss: 0.04434\n",
      "[Evaluate]  dev score: 0.82680, dev loss: 0.70835\n",
      "[Train] epoch: 2/3, step: 480/588, loss: 0.00724\n",
      "[Evaluate]  dev score: 0.83304, dev loss: 0.65254\n",
      "[Train] epoch: 2/3, step: 490/588, loss: 0.00134\n",
      "[Evaluate]  dev score: 0.83440, dev loss: 0.68457\n",
      "[Train] epoch: 2/3, step: 500/588, loss: 0.02356\n",
      "[Evaluate]  dev score: 0.83216, dev loss: 0.74744\n",
      "[Train] epoch: 2/3, step: 510/588, loss: 0.02422\n",
      "[Evaluate]  dev score: 0.83384, dev loss: 0.52092\n",
      "[Train] epoch: 2/3, step: 520/588, loss: 0.06381\n",
      "[Evaluate]  dev score: 0.81456, dev loss: 0.65805\n",
      "[Train] epoch: 2/3, step: 530/588, loss: 0.00453\n",
      "[Evaluate]  dev score: 0.81696, dev loss: 0.74306\n",
      "[Train] epoch: 2/3, step: 540/588, loss: 0.01108\n",
      "[Evaluate]  dev score: 0.82096, dev loss: 0.70869\n",
      "[Train] epoch: 2/3, step: 550/588, loss: 0.00335\n",
      "[Evaluate]  dev score: 0.81448, dev loss: 0.86684\n",
      "[Train] epoch: 2/3, step: 560/588, loss: 0.02346\n",
      "[Evaluate]  dev score: 0.83096, dev loss: 0.68447\n",
      "[Train] epoch: 2/3, step: 570/588, loss: 0.00554\n",
      "[Evaluate]  dev score: 0.83048, dev loss: 0.55973\n",
      "[Train] epoch: 2/3, step: 580/588, loss: 0.00633\n",
      "[Evaluate]  dev score: 0.82816, dev loss: 0.58361\n",
      "[Evaluate]  dev score: 0.82680, dev loss: 0.70307\n",
      "[Train] Training done!\n",
      "time:  83.26710295677185\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "paddle.seed(0)\r\n",
    "\r\n",
    "# 指定训练轮次\r\n",
    "num_epochs = 3\r\n",
    "# 指定学习率\r\n",
    "learning_rate = 0.001\r\n",
    "# 指定embedding的数量为词表长度\r\n",
    "num_embeddings = len(word2id_dict)\r\n",
    "# embedding向量的维度\r\n",
    "input_size = 256\r\n",
    "# LSTM网络隐状态向量的维度\r\n",
    "hidden_size = 256\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = Model_MultiLayer_BiLSTM_FC(num_embeddings, input_size, hidden_size)\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= model.parameters()) \r\n",
    "# 指定损失函数\r\n",
    "loss_fn = paddle.nn.CrossEntropyLoss() \r\n",
    "# 指定评估指标\r\n",
    "metric = Accuracy()\r\n",
    "# 实例化Runner\r\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\r\n",
    "# 模型训练\r\n",
    "start_time = time.time()\r\n",
    "runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=10, log_steps=10, save_path=\"./checkpoints/best_multilayer.pdparams\")\r\n",
    "end_time = time.time()\r\n",
    "print(\"time: \", (end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set, Accuracy: 0.85808\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./checkpoints/best_multilayer.pdparams\"\r\n",
    "runner.load_model(model_path)\r\n",
    "accuracy, _ =  runner.evaluate(test_loader)\r\n",
    "print(f\"Evaluate on test set, Accuracy: {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "观察到单层LSTM在测试集上达到了**0.82880**的准确率，2层的LSTM达到了**0.85808**的准确率，堆叠层数一定程度上提高了模型的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.本节实现了单向的LSTM模型，请思考如何实现双向LSTM，并基于IMDB数据集进行文本分类任务。<span style=\"color:red\">(附加题&加分题)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model_BiDirect_BiLSTM_FC(nn.Layer):\r\n",
    "    def __init__(self, num_embeddings, input_size, hidden_size, num_classes=2):\r\n",
    "        super(Model_BiDirect_BiLSTM_FC, self).__init__()\r\n",
    "        # 词典大小\r\n",
    "        self.num_embeddings = num_embeddings\r\n",
    "        # 单词向量的维度\r\n",
    "        self.input_size = input_size\r\n",
    "        # LSTM隐藏单元数量\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        # 情感分类类别数量\r\n",
    "        self.num_classes = num_classes\r\n",
    "        # 实例化嵌入层\r\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings, input_size, padding_idx=0)\r\n",
    "        # 实例化LSTM层\r\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, direction=\"bidirectional\")\r\n",
    "        # 实例化聚合层\r\n",
    "        self.average_layer = AveragePooling()\r\n",
    "        # 实例化输出层\r\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        # 对模型输入拆分为序列数据和mask\r\n",
    "        input_ids, sequence_length = inputs\r\n",
    "        # 获取词向量\r\n",
    "        inputs_emb = self.embedding_layer(input_ids)\r\n",
    "        # 使用lstm处理数据\r\n",
    "        sequence_output, _ = self.lstm_layer(inputs_emb, sequence_length=sequence_length)\r\n",
    "        # 使用聚合层聚合sequence_output\r\n",
    "        batch_mean_hidden = self.average_layer(sequence_output, sequence_length)\r\n",
    "        # 输出文本分类logits\r\n",
    "        logits = self.output_layer(batch_mean_hidden)\r\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/3, step: 0/588, loss: 0.69207\n",
      "[Train] epoch: 0/3, step: 10/588, loss: 0.69213\n",
      "[Evaluate]  dev score: 0.49472, dev loss: 0.69272\n",
      "[Evaluate] best accuracy performence has been updated: 0.00000 --> 0.49472\n",
      "[Train] epoch: 0/3, step: 20/588, loss: 0.68635\n",
      "[Evaluate]  dev score: 0.50888, dev loss: 0.68027\n",
      "[Evaluate] best accuracy performence has been updated: 0.49472 --> 0.50888\n",
      "[Train] epoch: 0/3, step: 30/588, loss: 0.44815\n",
      "[Evaluate]  dev score: 0.75936, dev loss: 0.51049\n",
      "[Evaluate] best accuracy performence has been updated: 0.50888 --> 0.75936\n",
      "[Train] epoch: 0/3, step: 40/588, loss: 0.30420\n",
      "[Evaluate]  dev score: 0.79784, dev loss: 0.46735\n",
      "[Evaluate] best accuracy performence has been updated: 0.75936 --> 0.79784\n",
      "[Train] epoch: 0/3, step: 50/588, loss: 0.35505\n",
      "[Evaluate]  dev score: 0.78016, dev loss: 0.45528\n",
      "[Train] epoch: 0/3, step: 60/588, loss: 0.27827\n",
      "[Evaluate]  dev score: 0.83672, dev loss: 0.37772\n",
      "[Evaluate] best accuracy performence has been updated: 0.79784 --> 0.83672\n",
      "[Train] epoch: 0/3, step: 70/588, loss: 0.39442\n",
      "[Evaluate]  dev score: 0.84104, dev loss: 0.36397\n",
      "[Evaluate] best accuracy performence has been updated: 0.83672 --> 0.84104\n",
      "[Train] epoch: 0/3, step: 80/588, loss: 0.26797\n",
      "[Evaluate]  dev score: 0.84336, dev loss: 0.36207\n",
      "[Evaluate] best accuracy performence has been updated: 0.84104 --> 0.84336\n",
      "[Train] epoch: 0/3, step: 90/588, loss: 0.30705\n",
      "[Evaluate]  dev score: 0.84976, dev loss: 0.35033\n",
      "[Evaluate] best accuracy performence has been updated: 0.84336 --> 0.84976\n",
      "[Train] epoch: 0/3, step: 100/588, loss: 0.36562\n",
      "[Evaluate]  dev score: 0.85040, dev loss: 0.34779\n",
      "[Evaluate] best accuracy performence has been updated: 0.84976 --> 0.85040\n",
      "[Train] epoch: 0/3, step: 110/588, loss: 0.29949\n",
      "[Evaluate]  dev score: 0.85272, dev loss: 0.34585\n",
      "[Evaluate] best accuracy performence has been updated: 0.85040 --> 0.85272\n",
      "[Train] epoch: 0/3, step: 120/588, loss: 0.33000\n",
      "[Evaluate]  dev score: 0.84616, dev loss: 0.35932\n",
      "[Train] epoch: 0/3, step: 130/588, loss: 0.25191\n",
      "[Evaluate]  dev score: 0.85648, dev loss: 0.33837\n",
      "[Evaluate] best accuracy performence has been updated: 0.85272 --> 0.85648\n",
      "[Train] epoch: 0/3, step: 140/588, loss: 0.25718\n",
      "[Evaluate]  dev score: 0.85200, dev loss: 0.34135\n",
      "[Train] epoch: 0/3, step: 150/588, loss: 0.40651\n",
      "[Evaluate]  dev score: 0.84264, dev loss: 0.35763\n",
      "[Train] epoch: 0/3, step: 160/588, loss: 0.25886\n",
      "[Evaluate]  dev score: 0.82904, dev loss: 0.37886\n",
      "[Train] epoch: 0/3, step: 170/588, loss: 0.28161\n",
      "[Evaluate]  dev score: 0.84280, dev loss: 0.36315\n",
      "[Train] epoch: 0/3, step: 180/588, loss: 0.28110\n",
      "[Evaluate]  dev score: 0.85504, dev loss: 0.34111\n",
      "[Train] epoch: 0/3, step: 190/588, loss: 0.18416\n",
      "[Evaluate]  dev score: 0.86008, dev loss: 0.33098\n",
      "[Evaluate] best accuracy performence has been updated: 0.85648 --> 0.86008\n",
      "[Train] epoch: 1/3, step: 200/588, loss: 0.13087\n",
      "[Evaluate]  dev score: 0.85936, dev loss: 0.33031\n",
      "[Train] epoch: 1/3, step: 210/588, loss: 0.11675\n",
      "[Evaluate]  dev score: 0.85168, dev loss: 0.45850\n",
      "[Train] epoch: 1/3, step: 220/588, loss: 0.09244\n",
      "[Evaluate]  dev score: 0.85280, dev loss: 0.37655\n",
      "[Train] epoch: 1/3, step: 230/588, loss: 0.05073\n",
      "[Evaluate]  dev score: 0.84424, dev loss: 0.43433\n",
      "[Train] epoch: 1/3, step: 240/588, loss: 0.10061\n",
      "[Evaluate]  dev score: 0.84136, dev loss: 0.51180\n",
      "[Train] epoch: 1/3, step: 250/588, loss: 0.01657\n",
      "[Evaluate]  dev score: 0.84816, dev loss: 0.39936\n",
      "[Train] epoch: 1/3, step: 260/588, loss: 0.15038\n",
      "[Evaluate]  dev score: 0.84400, dev loss: 0.40216\n",
      "[Train] epoch: 1/3, step: 270/588, loss: 0.06123\n",
      "[Evaluate]  dev score: 0.84552, dev loss: 0.44253\n",
      "[Train] epoch: 1/3, step: 280/588, loss: 0.08976\n",
      "[Evaluate]  dev score: 0.84408, dev loss: 0.39539\n",
      "[Train] epoch: 1/3, step: 290/588, loss: 0.05078\n",
      "[Evaluate]  dev score: 0.84416, dev loss: 0.39388\n",
      "[Train] epoch: 1/3, step: 300/588, loss: 0.08405\n",
      "[Evaluate]  dev score: 0.83928, dev loss: 0.52227\n",
      "[Train] epoch: 1/3, step: 310/588, loss: 0.05125\n",
      "[Evaluate]  dev score: 0.84192, dev loss: 0.47527\n",
      "[Train] epoch: 1/3, step: 320/588, loss: 0.07736\n",
      "[Evaluate]  dev score: 0.83280, dev loss: 0.45623\n",
      "[Train] epoch: 1/3, step: 330/588, loss: 0.05063\n",
      "[Evaluate]  dev score: 0.83496, dev loss: 0.41920\n",
      "[Train] epoch: 1/3, step: 340/588, loss: 0.10671\n",
      "[Evaluate]  dev score: 0.83416, dev loss: 0.49210\n",
      "[Train] epoch: 1/3, step: 350/588, loss: 0.05414\n",
      "[Evaluate]  dev score: 0.82536, dev loss: 0.48341\n",
      "[Train] epoch: 1/3, step: 360/588, loss: 0.09270\n",
      "[Evaluate]  dev score: 0.82760, dev loss: 0.46923\n",
      "[Train] epoch: 1/3, step: 370/588, loss: 0.07062\n",
      "[Evaluate]  dev score: 0.83360, dev loss: 0.49927\n",
      "[Train] epoch: 1/3, step: 380/588, loss: 0.05582\n",
      "[Evaluate]  dev score: 0.83488, dev loss: 0.42578\n",
      "[Train] epoch: 1/3, step: 390/588, loss: 0.11711\n",
      "[Evaluate]  dev score: 0.82648, dev loss: 0.46598\n",
      "[Train] epoch: 2/3, step: 400/588, loss: 0.00352\n",
      "[Evaluate]  dev score: 0.82976, dev loss: 0.58870\n",
      "[Train] epoch: 2/3, step: 410/588, loss: 0.00404\n",
      "[Evaluate]  dev score: 0.79752, dev loss: 1.05701\n",
      "[Train] epoch: 2/3, step: 420/588, loss: 0.00375\n",
      "[Evaluate]  dev score: 0.82976, dev loss: 0.71618\n",
      "[Train] epoch: 2/3, step: 430/588, loss: 0.02180\n",
      "[Evaluate]  dev score: 0.81160, dev loss: 0.74416\n",
      "[Train] epoch: 2/3, step: 440/588, loss: 0.01167\n",
      "[Evaluate]  dev score: 0.80112, dev loss: 0.81152\n",
      "[Train] epoch: 2/3, step: 450/588, loss: 0.01098\n",
      "[Evaluate]  dev score: 0.83408, dev loss: 0.58705\n",
      "[Train] epoch: 2/3, step: 460/588, loss: 0.02727\n",
      "[Evaluate]  dev score: 0.82824, dev loss: 0.69024\n",
      "[Train] epoch: 2/3, step: 470/588, loss: 0.04432\n",
      "[Evaluate]  dev score: 0.82680, dev loss: 0.70829\n",
      "[Train] epoch: 2/3, step: 480/588, loss: 0.00724\n",
      "[Evaluate]  dev score: 0.83304, dev loss: 0.65254\n",
      "[Train] epoch: 2/3, step: 490/588, loss: 0.00133\n",
      "[Evaluate]  dev score: 0.83440, dev loss: 0.68458\n",
      "[Train] epoch: 2/3, step: 500/588, loss: 0.02354\n",
      "[Evaluate]  dev score: 0.83216, dev loss: 0.74736\n",
      "[Train] epoch: 2/3, step: 510/588, loss: 0.02422\n",
      "[Evaluate]  dev score: 0.83384, dev loss: 0.52095\n",
      "[Train] epoch: 2/3, step: 520/588, loss: 0.06381\n",
      "[Evaluate]  dev score: 0.81456, dev loss: 0.65805\n",
      "[Train] epoch: 2/3, step: 530/588, loss: 0.00453\n",
      "[Evaluate]  dev score: 0.81688, dev loss: 0.74307\n",
      "[Train] epoch: 2/3, step: 540/588, loss: 0.01108\n",
      "[Evaluate]  dev score: 0.82096, dev loss: 0.70864\n",
      "[Train] epoch: 2/3, step: 550/588, loss: 0.00335\n",
      "[Evaluate]  dev score: 0.81448, dev loss: 0.86677\n",
      "[Train] epoch: 2/3, step: 560/588, loss: 0.02345\n",
      "[Evaluate]  dev score: 0.83096, dev loss: 0.68448\n",
      "[Train] epoch: 2/3, step: 570/588, loss: 0.00554\n",
      "[Evaluate]  dev score: 0.83056, dev loss: 0.55983\n",
      "[Train] epoch: 2/3, step: 580/588, loss: 0.00633\n",
      "[Evaluate]  dev score: 0.82816, dev loss: 0.58364\n",
      "[Evaluate]  dev score: 0.82664, dev loss: 0.70301\n",
      "[Train] Training done!\n",
      "time:  81.58658003807068\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "paddle.seed(0)\r\n",
    "\r\n",
    "# 指定训练轮次\r\n",
    "num_epochs = 3\r\n",
    "# 指定学习率\r\n",
    "learning_rate = 0.001\r\n",
    "# 指定embedding的数量为词表长度\r\n",
    "num_embeddings = len(word2id_dict)\r\n",
    "# embedding向量的维度\r\n",
    "input_size = 256\r\n",
    "# LSTM网络隐状态向量的维度\r\n",
    "hidden_size = 256\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = Model_MultiLayer_BiLSTM_FC(num_embeddings, input_size, hidden_size)\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= model.parameters()) \r\n",
    "# 指定损失函数\r\n",
    "loss_fn = paddle.nn.CrossEntropyLoss() \r\n",
    "# 指定评估指标\r\n",
    "metric = Accuracy()\r\n",
    "# 实例化Runner\r\n",
    "runner = RunnerV3(model, optimizer, loss_fn, metric)\r\n",
    "# 模型训练\r\n",
    "start_time = time.time()\r\n",
    "runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=10, log_steps=10, save_path=\"./checkpoints/best_bidirect.pdparams\")\r\n",
    "end_time = time.time()\r\n",
    "print(\"time: \", (end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set, Accuracy: 0.85816\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./checkpoints/best_bidirect.pdparams\"\r\n",
    "runner.load_model(model_path)\r\n",
    "accuracy, _ =  runner.evaluate(test_loader)\r\n",
    "print(f\"Evaluate on test set, Accuracy: {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "观察到单层LSTM在测试集上达到了**0.82880**的准确率，2层的LSTM达到了**0.85808**的准确率，双向单层LSTM则达到了**0.85816**的准确率，比2层LSTM略高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. <span style=\"color:red\">(附加题&简答题&加分题)</span>\n",
    "    小明刚刚学习了循环神经网络，觉得这个网络非常有实际用途。正巧这几天天气反复多变，气温忽冷忽热。因此小明想是否可以用LSTM去预测什么时候天气能够持续回暖。\n",
    "    小明在网上搜索，找到了往前三年的气温数据，气温数据每个1小时就会被记录一次。 因此小明基于这份数据，使用LSTM进行建模，预测后续的气温情况。\n",
    "     小明发现通过设置数据步长为2，即使用前1小时的气温预测下1小时的气温，能够获得非常高的准确度。因此小明觉得模型已经训练得非常好了。\n",
    "    因此，他想把上一个时刻预测出的气温，作为下一个时刻的输入，依次向前预测，直到预测出往后3天的气温。 \n",
    "    大家觉得，小明能够准确获得第3天的气温吗？为什么？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "不一定。\n",
    "使用前1小时的气温预测下1小时的气温，即是使用teacher forcing的方式训练，但从当前时刻开始，每次的输入都是上一时间步预测出的输出，依次往下预测会不可避免地出现错误传播，所以我认为不太可能准确获得第3天的气温。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
